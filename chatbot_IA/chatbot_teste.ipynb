{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62081487-8b70-4d41-ac22-db99732eafae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk # pip install nltk\n",
    "import re # pip install re\n",
    "import string\n",
    "import spacy #pip install spacy\n",
    "\n",
    "from spacy.lang.pt.examples import sentences\n",
    "from nltk.chat.util import Chat, reflections\n",
    "from bs4 import BeautifulSoup # pip install bs4\n",
    "from urllib.request import urlopen\n",
    "from random import choice, randint\n",
    "from sklearn.metrics.pairwise import cosine_similarity # pip install scikit-learn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # pip install scikit-learn\n",
    "\n",
    "# pip install lxml => erro de parser\n",
    "# python -m spacy download pt_core_news_sm => erro de linguagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16d13a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_url = urlopen(\"https://pt.wikipedia.org/wiki/Intelig%C3%AAncia_artificial\").read()\n",
    "txt_html = BeautifulSoup(txt_url, 'lxml').find_all('p')\n",
    "texto_pag_web = ''\n",
    "for i in txt_html:\n",
    "  texto_pag_web += i.text.lower()\n",
    "lista_sentencas = nltk.sent_tokenize(texto_pag_web)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b156bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"pt_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5244b987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# palavras que o modelo irá ignorar\n",
    "stop_words = spacy.lang.pt.stop_words.STOP_WORDS\n",
    "# pontuações que o modelo irá ignorar\n",
    "stop_punct = string.punctuation\n",
    "\n",
    "def preprocessamento(texto): #preparando o texto para ser processado pelo spacy\n",
    "  # tirar urls\n",
    "  texto = re.sub(r\"https?://[A-Za-z0-9./]+\",' ',texto)\n",
    "  # tirar espaços em branco\n",
    "  texto = re.sub(r\" +\", ' ',texto)\n",
    "  # tirar radical (lematização)\n",
    "  documento = nlp(texto)\n",
    "  lista = []\n",
    "  for token in documento:\n",
    "    lista.append(token.lemma_)\n",
    "  lista = [palavra for palavra in lista if palavra not in stop_words and palavra not in stop_punct]\n",
    "  lista = ' '.join([str(elemento) for elemento in lista if not elemento.isdigit()])\n",
    "  return lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74647cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardar as sentenças que serão pré-processadas pela função em uma lista\n",
    "lista_sentencas_preprocessada = []\n",
    "for i in range(len(lista_sentencas)):\n",
    "  lista_sentencas_preprocessada.append(preprocessamento(lista_sentencas[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13139214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparando o texto para os calculos de TF e IDF\n",
    "teste = lista_sentencas_preprocessada[:3]\n",
    "teste.append(teste[0])\n",
    "vetores = TfidfVectorizer()\n",
    "# a função fit_transform faz todos os calculos de TF e IDF\n",
    "palavras_vetorizadas = vetores.fit_transform(teste)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc27026",
   "metadata": {},
   "outputs": [],
   "source": [
    "palavras_vetorizadas.todense().shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
